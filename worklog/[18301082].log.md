

## **7.5**

**AM**

1.Flask配置

比较简单，没有问题

2.Flask学习

有点没头绪，等下午继续学习

**PM**

1.PPT制作

明天答辩，简单地做了一下PPT，展示已完成内容

2.Flask学习

## **7.4**

**AM**

1.全年数据清洗

更改了数据清洗代码，完成了1981年到2012年（因为12年之后数据缺失项多）的每一天的数据清洗，并写入相应的csv文件

2.创建Python处理类，生成json格式数据

较为顺利



**PM**

1.json数据标准化

修改预测数据的日期由12-31为对应日期，并生成预测图和标准json文件

2.开会分享进度

进度比较缓慢，尤其是前端方面比较迷茫，花了大量时间学习但进展很慢。主要原因还是对于前端web网页和服务器之间的连接部分的知识缺失，也无从下手。暂且打算多问问其他组的想法加上自我的学习。

## **7.3**

**AM**

1.数据清洗

其实昨天就做了，今天对其的理解更深刻。简单地对csv文件进行了清洗，获得了只有DATE、TMAX、TMIN的csv文件。学会了对数据的筛选和过滤空项。

2.ARIMA模型研究

对ARIMA模型的继续研究，较为明白地理解了其原理和检验相关内容。

**PM**

1.初步建立基于时间序列的模型

基本掌握了模型的使用方法，部分原理，检验标准等。但是遇到了一个问题，即模型的p  q只能人工肉眼确定，误差大。故转而寻找网上相应代码，并成功使用BIC来进行检验确定最佳的p q值。但同时涌现新的问题，基本p q都是在取0 1 2时，BIC最小，自我认为有一定的问题，留待明日研究。

## **7.2**

**AM**

1.老师进行小组抽查进度

​		我借此了解别的组的进度和对项目的理解和分工。

2.配置虚拟机

​		配置时发现开启三台虚拟机时电脑无法正常运行，故让小组成员在被提问时，询问spark集群的意义。得到结论：【电脑开启三台虚拟机，使用spark集群是为了提高处理数据的能力和效率，（类似饭店一次请了3个厨师QAQ）。然而由于笔记本电脑配置的原因，无法正常运行，故反而影响效率】所以最终我们组决定暂时使用windows下单节点处理数据。而这个数据也只是任务实现上要求的北京的气温数据，暂时不动老师给的100G数据，留待我们完成1次迭代后，拓展时研究。

**PM**

1.老师讲解任务之间的关系

​		终于了解到任务建的关系。尤其是flask、json和Tomcat等之间的联系。目前我们组的理解是，flask是用于python传出预测的数据，json是传输数据的格式，tomact用作调用python的微程序接口获得预测的数据。

2.小组任务再明确

​	由于对之后几个任务的不明确，我们的前端开发小组成员不清楚从哪里属于前端的任务，故进度停滞。再明确后，确立他们先学习HTML5和CSS，再做出一个初步的web网页。前后端对接将由我们完成各自任务后再共同解决。

3.ARIMA模型初步学习

​	还不甚理解，晚上继续

## **7.1**

**感想**

​	甚是惭愧，身为小组长今天才在配置虚拟机的时候第一次发现，我因为对整个项目的不了解一	直对项目的分工很粗略，两天来一直放任组员自由配置，然后出现问题帮助解决，时间花费巨	大而项目进程推进缓慢，小组成员也较为迷惘。所以今日我仔细研究了项目进程，询问了多个	同学（实名感谢张璞、童路勤、何文龙），另外也观摩了隔壁班别的小组的会议，对项目有了	较为宏观的总体认识。最终在这个下午我对项目任务进行了分解和再分配。暂时本周任务具体	分为

​	【spark数据清洗】王昱、张正阳

​	【前端设计】赵康辉、林小倩

​	【模型设计】武熠彬（王昱、张正阳在掌握spark数据清洗的技术后加入研究模型设计，因为我觉得这点是最难理解，也是最需要突破的地方）

​	望老师多加指正！（我第一次当小组长，小组成员和我也不够“大腿”没有足够的经验）

**AM**

1.重新下载安装新版ubuntu。

​	配置网络时由于找不到pdf所示的edit connections选项，故停滞许久。然后从同学处了解到新版	无需配置Ipv4,。之后在打开ssh服务处停滞，始终显示无etc\init.d命令。然之后发现无需手动打	开，他在下载后自动打开了。

2.下载Xshell并成功与虚拟机连接

​	无大障碍

**PM**

1.建立主节点master从节点slave1和slave2，并成功实现互ping

​	但是我的笔记本似乎不支持3个虚拟机同时打开，会卡住，我甚至直接黑屏重启了。

2.突然醒悟，开始从项目整体来看，并多方咨询，最终有所了解

​	具体见上文【感想】

3.组织小组讨论，进行任务重分配和疑惑解答

## **6.30.**

AM

1.创建worklog文件夹并上传至远程仓库

2.配置成功Python和Pycharm（jdk1.8本来就有）

3.配置Spark和hadoop，失败2次，似乎是版本问题，仍在尝试

PM

1.成功配置Spark和hadoop（重下并配置jdk1.8）

2.成功安装VMware Workstation

3.成功下载ubuntu文件并成功在VMware里安装，但还未配置



